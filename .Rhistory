fig.height = 6,
fig.align = 'center'
)
rm(list = ls())
knitr::opts_chunk$set(
echo = FALSE,
warning = FALSE,
message = FALSE,
fig.width = 8,
fig.height = 6,
fig.align = 'center'
)
# Load required packages
suppressPackageStartupMessages({
library(urca)         # Unit root and cointegration tests
library(vars)         # VAR/VECM models
library(rugarch)      # GARCH models
library(fGarch)       # Alternative GARCH
library(FinTS)        # ARCH-LM tests
library(forecast)     # ARMA model selection
library(dplyr)        # Data manipulation
library(tidyr)        # Data tidying
library(zoo)          # Time series objects
library(pracma)       # Detrending
library(ggplot2)      # Plotting
library(ggfortify)    # Time series plotting
library(gridExtra)    # Multiple plots
library(corrplot)     # Correlation plots
library(moments)      # Skewness and kurtosis
library(knitr)        # Tables
})
# Set random seed for reproducibility
set.seed(42)
# Load S&P 500 data
sp500 <- read.csv("Data/sp500.csv", stringsAsFactors = FALSE)
sp500$Date <- as.Date(sp500$Date, format = "%m/%d/%Y")
# Load EUR/USD data
eurusd <- read.csv("Data/EUR_USD.csv", stringsAsFactors = FALSE)
eurusd$Date <- as.Date(eurusd$Date, format = "%m/%d/%Y")
# Convert EUR/USD from daily to monthly (end-of-month values)
library(zoo)
eurusd_zoo <- zoo(eurusd$Value, order.by = eurusd$Date)
eurusd_monthly <- aggregate(eurusd_zoo, zoo::as.yearmon, tail, 1)
eurusd_monthly_df <- data.frame(
Date = zoo::as.Date(zoo::as.yearmon(index(eurusd_monthly)), frac = 1),
Value = as.numeric(coredata(eurusd_monthly))
)
# Filter S&P 500 from 1999 onwards and merge with EUR/USD
sp500_filtered <- sp500 %>%
filter(Date >= as.Date("1999-01-01"))
data_merged <- inner_join(
sp500_filtered %>% rename(SP500 = Value),
eurusd_monthly_df %>% rename(EURUSD = Value),
by = "Date"
)
# Calculate log returns using differencing
data_merged <- data_merged %>%
mutate(
r_SP500 = c(NA, diff(log(SP500))),
r_EURUSD = c(NA, diff(log(EURUSD)))
) %>%
filter(!is.na(r_SP500))
# Create bivariate time series object
returns_ts <- ts(
data_merged[, c("r_SP500", "r_EURUSD")],
start = c(1999, 2),
frequency = 12
)
# Calculate summary statistics
summary_stats <- data.frame(
Variable = c("S&P 500 Returns", "EUR/USD Returns"),
Mean = c(mean(returns_ts[, 1], na.rm = TRUE), mean(returns_ts[, 2], na.rm = TRUE)),
SD = c(sd(returns_ts[, 1], na.rm = TRUE), sd(returns_ts[, 2], na.rm = TRUE)),
Min = c(min(returns_ts[, 1], na.rm = TRUE), min(returns_ts[, 2], na.rm = TRUE)),
Max = c(max(returns_ts[, 1], na.rm = TRUE), max(returns_ts[, 2], na.rm = TRUE)),
Skewness = c(
skewness(returns_ts[, 1], na.rm = TRUE),
skewness(returns_ts[, 2], na.rm = TRUE)
),
Kurtosis = c(
kurtosis(returns_ts[, 1], na.rm = TRUE),
kurtosis(returns_ts[, 2], na.rm = TRUE)
)
)
kable(summary_stats, digits = 3, caption = "Summary Statistics of Monthly Returns")
# Calculate correlation matrix
cor_matrix <- cor(returns_ts, use = "complete.obs")
# Display correlation
kable(cor_matrix, digits = 3, caption = "Correlation Matrix of Returns")
# Define custom theme
theme_lecture <- theme_light() +
theme(
plot.margin = margin(0.2, 0.2, 0.2, 0.2, "cm"),
text = element_text(size = 12),
plot.title = element_text(hjust = 0.5, face = "bold"),
axis.title = element_text(face = "bold")
)
# Plot S&P 500 levels
ggplot(data_merged, aes(x = Date, y = SP500)) +
geom_line(color = "steelblue", linewidth = 0.7) +
labs(
title = "S&P 500 Index Levels",
x = "Date",
y = "Index Value"
) +
theme_lecture
# Plot S&P 500 returns
ggplot(data_merged, aes(x = Date, y = r_SP500)) +
geom_line(color = "darkred", linewidth = 0.5) +
geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
labs(
title = "S&P 500 Monthly Returns",
x = "Date",
y = "Log Returns"
) +
theme_lecture
# Plot EUR/USD levels
ggplot(data_merged, aes(x = Date, y = EURUSD)) +
geom_line(color = "darkgreen", linewidth = 0.7) +
labs(
title = "EUR/USD Exchange Rate Levels",
x = "Date",
y = "Exchange Rate"
) +
theme_lecture
# Plot EUR/USD returns
ggplot(data_merged, aes(x = Date, y = r_EURUSD)) +
geom_line(color = "darkorange", linewidth = 0.5) +
geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
labs(
title = "EUR/USD Monthly Returns",
x = "Date",
y = "Log Returns"
) +
theme_lecture
# Sequential ADF testing function with dynamic critical values
# Tests against 1%, then 5%, then 10% critical values sequentially
sequential_adf_test <- function(series, series_name, max_lags = 8) {
sig_levels <- c("1pct", "5pct", "10pct")
# Step 1: Test with trend
adf_trend <- ur.df(series, type = 'trend', lags = max_lags, selectlags = "AIC")
cat("\n===", series_name, "- ADF Test with Trend ===\n")
print(summary(adf_trend))
# Extract test statistics
tau3 <- adf_trend@teststat[1, "tau3"]
phi3 <- adf_trend@teststat[1, "phi3"]
# Check tau3 against critical values sequentially (1% -> 5% -> 10%)
for (sig in sig_levels) {
tau3_cv <- adf_trend@cval["tau3", sig]
if (tau3 < tau3_cv) {
return(list(conclusion = "I(0)", specification = "trend", test = adf_trend,
message = paste("τ₃ =", round(tau3, 3), "< CV =", round(tau3_cv, 3), "(", sig, ") → Reject H₀, series is I(0)")))
}
}
# Check phi3 against critical values sequentially (1% -> 5% -> 10%)
for (sig in sig_levels) {
phi3_cv <- adf_trend@cval["phi3", sig]
if (phi3 > phi3_cv) {
return(list(conclusion = "I(1)", specification = "trend", test = adf_trend,
message = paste("φ₃ =", round(phi3, 3), "> CV =", round(phi3_cv, 3), "(", sig, ") → Trend significant, keep specification. Series is I(1)")))
}
}
phi3_cv_10 <- adf_trend@cval["phi3", "10pct"]
cat("\nφ₃ =", round(phi3, 3), "< CV =", round(phi3_cv_10, 3), "(10pct) → Trend not needed, stepping down to drift\n")
# Step 2: Test with drift only
adf_drift <- ur.df(series, type = 'drift', lags = max_lags, selectlags = "AIC")
cat("\n===", series_name, "- ADF Test with Drift ===\n")
print(summary(adf_drift))
tau2 <- adf_drift@teststat[1, "tau2"]
phi1 <- adf_drift@teststat[1, "phi1"]
# Check tau2 against critical values sequentially (1% -> 5% -> 10%)
for (sig in sig_levels) {
tau2_cv <- adf_drift@cval["tau2", sig]
if (tau2 < tau2_cv) {
return(list(conclusion = "I(0)", specification = "drift", test = adf_drift,
message = paste("τ₂ =", round(tau2, 3), "< CV =", round(tau2_cv, 3), "(", sig, ") → Reject H₀, series is I(0)")))
}
}
# Check phi1 against critical values sequentially (1% -> 5% -> 10%)
for (sig in sig_levels) {
phi1_cv <- adf_drift@cval["phi1", sig]
if (phi1 > phi1_cv) {
return(list(conclusion = "I(1)", specification = "drift", test = adf_drift,
message = paste("φ₁ =", round(phi1, 3), "> CV =", round(phi1_cv, 3), "(", sig, ") → Drift significant, keep specification. Series is I(1)")))
}
}
phi1_cv_10 <- adf_drift@cval["phi1", "10pct"]
cat("\nφ₁ =", round(phi1, 3), "< CV =", round(phi1_cv_10, 3), "(10pct) → Drift not needed, stepping down to none\n")
# Step 3: Test with no deterministic components
adf_none <- ur.df(series, type = 'none', lags = max_lags, selectlags = "AIC")
cat("\n===", series_name, "- ADF Test with None ===\n")
print(summary(adf_none))
tau1 <- adf_none@teststat[1, "tau1"]
# Check tau1 against critical values sequentially (1% -> 5% -> 10%)
for (sig in sig_levels) {
tau1_cv <- adf_none@cval["tau1", sig]
if (tau1 < tau1_cv) {
return(list(conclusion = "I(0)", specification = "none", test = adf_none,
message = paste("τ₁ =", round(tau1, 3), "< CV =", round(tau1_cv, 3), "(", sig, ") → Reject H₀, series is I(0)")))
}
}
tau1_cv_10 <- adf_none@cval["tau1", "10pct"]
return(list(conclusion = "I(1)", specification = "none", test = adf_none,
message = paste("τ₁ =", round(tau1, 3), "> CV =", round(tau1_cv_10, 3), "(10pct) → Fail to reject H₀, series is I(1)")))
}
# Sequential ADF test for returns (starts from drift, skips trend)
# Used for already-differenced data where trend was rejected in levels
# Tests against 1%, then 5%, then 10% critical values sequentially
sequential_adf_test_returns <- function(series, series_name, max_lags = 8) {
sig_levels <- c("1pct", "5pct", "10pct")
# Step 1: Test with drift (skip trend for differenced data)
adf_drift <- ur.df(series, type = 'drift', lags = max_lags, selectlags = "AIC")
cat("\n===", series_name, "- ADF Test with Drift ===\n")
print(summary(adf_drift))
tau2 <- adf_drift@teststat[1, "tau2"]
phi1 <- adf_drift@teststat[1, "phi1"]
# Check tau2 against critical values sequentially (1% -> 5% -> 10%)
for (sig in sig_levels) {
tau2_cv <- adf_drift@cval["tau2", sig]
if (tau2 < tau2_cv) {
return(list(conclusion = "I(0)", specification = "drift", test = adf_drift,
message = paste("τ₂ =", round(tau2, 3), "< CV =", round(tau2_cv, 3), "(", sig, ") → Reject H₀, series is I(0)")))
}
}
# Check phi1 against critical values sequentially (1% -> 5% -> 10%)
for (sig in sig_levels) {
phi1_cv <- adf_drift@cval["phi1", sig]
if (phi1 > phi1_cv) {
return(list(conclusion = "I(1)", specification = "drift", test = adf_drift,
message = paste("φ₁ =", round(phi1, 3), "> CV =", round(phi1_cv, 3), "(", sig, ") → Drift significant, keep specification. Series is I(1)")))
}
}
phi1_cv_10 <- adf_drift@cval["phi1", "10pct"]
cat("\nφ₁ =", round(phi1, 3), "< CV =", round(phi1_cv_10, 3), "(10pct) → Drift not needed, stepping down to none\n")
# Step 2: Test with no deterministic components
adf_none <- ur.df(series, type = 'none', lags = max_lags, selectlags = "AIC")
cat("\n===", series_name, "- ADF Test with None ===\n")
print(summary(adf_none))
tau1 <- adf_none@teststat[1, "tau1"]
# Check tau1 against critical values sequentially (1% -> 5% -> 10%)
for (sig in sig_levels) {
tau1_cv <- adf_none@cval["tau1", sig]
if (tau1 < tau1_cv) {
return(list(conclusion = "I(0)", specification = "none", test = adf_none,
message = paste("τ₁ =", round(tau1, 3), "< CV =", round(tau1_cv, 3), "(", sig, ") → Reject H₀, series is I(0)")))
}
}
tau1_cv_10 <- adf_none@cval["tau1", "10pct"]
return(list(conclusion = "I(1)", specification = "none", test = adf_none,
message = paste("τ₁ =", round(tau1, 3), "> CV =", round(tau1_cv_10, 3), "(10pct) → Fail to reject H₀, series is I(1)")))
}
result_sp500_levels <- sequential_adf_test(data_merged$SP500, "S&P 500 Levels")
cat("\n*** CONCLUSION:", result_sp500_levels$message, "***\n")
result_eurusd_levels <- sequential_adf_test(data_merged$EURUSD, "EUR/USD Levels")
cat("\n*** CONCLUSION:", result_eurusd_levels$message, "***\n")
result_sp500_returns <- sequential_adf_test_returns(returns_ts[, "r_SP500"], "S&P 500 Log Returns")
cat("\n*** CONCLUSION:", result_sp500_returns$message, "***\n")
result_eurusd_returns <- sequential_adf_test_returns(returns_ts[, "r_EURUSD"], "EUR/USD Log Returns")
cat("\n*** CONCLUSION:", result_eurusd_returns$message, "***\n")
# Create log of price levels (the I(1) series)
log_sp500 <- log(data_merged$SP500)
log_eurusd <- log(data_merged$EURUSD)
# Create time series objects
log_sp500_ts <- ts(log_sp500, start = c(1999, 2), frequency = 12)
log_eurusd_ts <- ts(log_eurusd, start = c(1999, 2), frequency = 12)
# Combine into multivariate time series
log_levels <- ts.intersect(log_sp500_ts, log_eurusd_ts)
# Plot both series together
ts.plot(log_levels, lty = c(1, 2), lwd = c(2, 2),
col = c("steelblue", "darkred"),
xlab = "Year", ylab = "Log Price Level",
main = "Log Price Levels: S&P 500 and EUR/USD")
legend("topleft", legend = c("log(S&P 500)", "log(EUR/USD)"),
col = c("steelblue", "darkred"), lty = c(1, 2), lwd = 2)
# Step 1: Estimate long-run relationship using OLS on log levels (I(1) series)
log_sp500 <- log(data_merged$SP500)
log_eurusd <- log(data_merged$EURUSD)
coint_reg <- lm(log_sp500 ~ log_eurusd)
# Display regression results
cat("=== Step 1: Estimate Long-Run Relationship ===\n")
cat("Regression: log(S&P 500) = α + θ × log(EUR/USD) + Z\n\n")
print(summary(coint_reg))
# Extract coefficients
alpha_coint <- coef(coint_reg)[1]
theta_coint <- coef(coint_reg)[2]
cat("\nEstimated long-run relationship:\n")
cat("log(S&P 500) =", round(alpha_coint, 4), "+", round(theta_coint, 4), "× log(EUR/USD)\n")
# Extract residuals
residuals_coint <- coint_reg$residuals
# Plot residuals
plot(data_merged$Date, residuals_coint, type = "l",
main = "Cointegrating Residuals",
xlab = "Date", ylab = "Residuals", col = "darkblue", lwd = 1.5)
abline(h = 0, lty = 2, col = "red")
# Step 2: Test residuals for unit root (Engle-Granger test)
# For cointegration residuals, we use type = "none" (no intercept or trend)
# because the residuals have zero mean by construction
cat("=== Step 2: ADF Test on Cointegrating Residuals ===\n\n")
adf_residuals <- ur.df(residuals_coint, type = "none", lags = 8, selectlags = "AIC")
print(summary(adf_residuals))
# Extract test statistic
tau_residuals <- adf_residuals@teststat[1, "tau1"]
# Get sample size for MacKinnon response surface formula
T_sample <- length(residuals_coint)
# MacKinnon (2010) response surface coefficients for cointegration
# k=2 variables, no trend (case 1: no constant in cointegrating regression)
# Formula: CV = beta_inf + beta_1/T + beta_2/T^2
mackinnon_coef <- data.frame(
level = c("1%", "5%", "10%"),
beta_inf = c(-3.9001, -3.3377, -3.0462),
beta_1 = c(-10.534, -5.967, -4.069),
beta_2 = c(-30.03, -8.98, -5.73)
)
# Compute sample-size adjusted critical values
cv_1pct <- mackinnon_coef$beta_inf[1] + mackinnon_coef$beta_1[1]/T_sample + mackinnon_coef$beta_2[1]/T_sample^2
cv_5pct <- mackinnon_coef$beta_inf[2] + mackinnon_coef$beta_1[2]/T_sample + mackinnon_coef$beta_2[2]/T_sample^2
cv_10pct <- mackinnon_coef$beta_inf[3] + mackinnon_coef$beta_1[3]/T_sample + mackinnon_coef$beta_2[3]/T_sample^2
mackinnon_cv <- data.frame(
Significance = c("1%", "5%", "10%"),
Critical_Value = c(cv_1pct, cv_5pct, cv_10pct)
)
cat("\n=== Cointegration Test Results ===\n")
cat("Sample size T =", T_sample, "\n")
cat("ADF test statistic on residuals (τ):", round(tau_residuals, 4), "\n\n")
cat("MacKinnon Critical Values (adjusted for sample size):\n")
cat("Formula: CV = β∞ + β₁/T + β₂/T²\n\n")
print(mackinnon_cv, row.names = FALSE)
# Determine conclusion using computed critical values
if (tau_residuals < cv_1pct) {
coint_conclusion <- paste0("Reject H₀ at 1% level: Series ARE cointegrated (τ = ",
round(tau_residuals, 4), " < ", round(cv_1pct, 4), ")")
coint_result <- "cointegrated"
} else if (tau_residuals < cv_5pct) {
coint_conclusion <- paste0("Reject H₀ at 5% level: Series ARE cointegrated (τ = ",
round(tau_residuals, 4), " < ", round(cv_5pct, 4), ")")
coint_result <- "cointegrated"
} else if (tau_residuals < cv_10pct) {
coint_conclusion <- paste0("Reject H₀ at 10% level: Series ARE cointegrated (τ = ",
round(tau_residuals, 4), " < ", round(cv_10pct, 4), ")")
coint_result <- "cointegrated"
} else {
coint_conclusion <- paste0("Fail to reject H₀: Series are NOT cointegrated (τ = ",
round(tau_residuals, 4), " > ", round(cv_10pct, 4), ")")
coint_result <- "not cointegrated"
}
cat("\n*** CONCLUSION:", coint_conclusion, "***\n")
# Select optimal lag
var_select <- VARselect(returns_ts, lag.max = 8, type = "const")
# Display information criteria
kable(t(var_select$criteria), digits = 3,
caption = "VAR Lag Selection Criteria",
col.names = c("AIC", "HQ", "SC", "FPE"))
# Extract optimal lag using AIC
p_optimal <- var_select$selection["AIC(n)"]
var_select$selection
p_optimal
# Estimate VAR model
var_model <- VAR(returns_ts, p = p_optimal, type = "const")
# Display summary
summary(var_model)
# Extract key statistics for summary table
r2_sp500 <- summary(var_model)$varresult$r_SP500$r.squared
r2_eurusd <- summary(var_model)$varresult$r_EURUSD$r.squared
adj_r2_sp500 <- summary(var_model)$varresult$r_SP500$adj.r.squared
adj_r2_eurusd <- summary(var_model)$varresult$r_EURUSD$adj.r.squared
fstat_sp500 <- summary(var_model)$varresult$r_SP500$fstatistic[1]
fstat_eurusd <- summary(var_model)$varresult$r_EURUSD$fstatistic[1]
var_summary <- data.frame(
Equation = c("S&P 500 Returns", "EUR/USD Returns"),
R_squared = c(r2_sp500, r2_eurusd),
Adj_R_squared = c(adj_r2_sp500, adj_r2_eurusd),
F_statistic = c(fstat_sp500, fstat_eurusd)
)
kable(var_summary, digits = 3,
caption = "VAR Model Summary Statistics",
col.names = c("Equation", "R²", "Adj. R²", "F-statistic"))
# Extract VAR residuals
var_residuals <- residuals(var_model)
# ARCH-LM test for S&P 500 residuals
arch_test_sp500 <- ArchTest(var_residuals[, "r_SP500"], lags = 12)
# ARCH-LM test for EUR/USD residuals
arch_test_eurusd <- ArchTest(var_residuals[, "r_EURUSD"], lags = 12)
# Create results table
arch_results <- data.frame(
Variable = c("S&P 500 Residuals", "EUR/USD Residuals"),
Test_Statistic = c(
arch_test_sp500$statistic,
arch_test_eurusd$statistic
),
p_value = c(
arch_test_sp500$p.value,
arch_test_eurusd$p.value
),
Conclusion = c(
ifelse(arch_test_sp500$p.value < 0.05, "ARCH effects present", "No ARCH effects"),
ifelse(arch_test_eurusd$p.value < 0.05, "ARCH effects present", "No ARCH effects")
)
)
kable(arch_results, digits = 4,
caption = "ARCH-LM Test Results on VAR Residuals",
col.names = c("Variable", "Chi-squared", "p-value", "Conclusion"))
# Automatic ARMA selection
arma_model <- auto.arima(
returns_ts[, "r_SP500"],
max.p = 20,
max.q = 20,
seasonal = FALSE,
stationary = TRUE,
ic = "aic"
)
# Extract ARMA orders
p_arma <- arma_model$arma[1]
q_arma <- arma_model$arma[2]
cat("Selected ARMA order: ARMA(", p_arma, ",", q_arma, ")\n", sep = "")
# Automatic ARMA selection
arma_model <- auto.arima(
returns_ts[, "r_SP500"],
max.p = 20,
max.q = 20,
seasonal = FALSE,
stationary = TRUE,
ic = "aic"
)
# Extract ARMA orders
p_arma <- arma_model$arma[1]
q_arma <- arma_model$arma[2]
cat("Selected ARMA order: ARMA(", p_arma, ",", q_arma, ")\n", sep = "")
# Specify GARCH(1,1) model
garch_spec <- ugarchspec(
variance.model = list(
model = "sGARCH",
garchOrder = c(1, 1)
),
mean.model = list(
armaOrder = c(p_arma, q_arma),
include.mean = TRUE
),
distribution.model = "norm"
)
# Estimate GARCH model
garch_fit <- ugarchfit(
spec = garch_spec,
data = returns_ts[, "r_SP500"]
)
# Display results
garch_fit
# Extract coefficients
garch_coef <- coef(garch_fit)
alpha0 <- garch_coef["omega"]
alpha1 <- garch_coef["alpha1"]
beta1 <- garch_coef["beta1"]
persistence <- alpha1 + beta1
# Calculate half-life of volatility shock
half_life <- log(0.5) / log(persistence)
# Create GARCH parameter table
garch_param_table <- data.frame(
Parameter = c("ω (omega)", "α₁ (alpha1)", "β₁ (beta1)", "Persistence (α₁ + β₁)", "Half-life (months)"),
Estimate = c(alpha0, alpha1, beta1, persistence, half_life),
Std_Error = c(
garch_fit@fit$matcoef["omega", "Std. Error"],
garch_fit@fit$matcoef["alpha1", "Std. Error"],
garch_fit@fit$matcoef["beta1", "Std. Error"],
NA,
NA
),
t_value = c(
garch_fit@fit$matcoef["omega", "t value"],
garch_fit@fit$matcoef["alpha1", "t value"],
garch_fit@fit$matcoef["beta1", "t value"],
NA,
NA
),
p_value = c(
garch_fit@fit$matcoef["omega", "Pr(>|t|)"],
garch_fit@fit$matcoef["alpha1", "Pr(>|t|)"],
garch_fit@fit$matcoef["beta1", "Pr(>|t|)"],
NA,
NA
)
)
# Extract conditional volatility (sigma_t)
cond_vol <- sigma(garch_fit)
# Create data frame for plotting
vol_data <- data.frame(
Date = data_merged$Date[-1],
Volatility = as.numeric(cond_vol)
)
# Extract standardized residuals
std_residuals <- residuals(garch_fit, standardize = TRUE)
# ARCH-LM test on standardized residuals
arch_test_std <- ArchTest(as.numeric(std_residuals), lags = 12)
cat("ARCH-LM test on standardized residuals:\n")
cat("Chi-squared statistic:", arch_test_std$statistic, "\n")
cat("p-value:", arch_test_std$p.value, "\n")
cat("Conclusion:", ifelse(arch_test_std$p.value > 0.05,
"No remaining ARCH effects (model adequate)",
"ARCH effects remain (consider alternative specification)"), "\n")
# Forecast volatility 5 periods ahead
garch_forecast <- ugarchforecast(
fitORspec = garch_fit,
n.ahead = 5
)
# Extract forecasted volatility
vol_forecast <- as.numeric(sigma(garch_forecast))
# Create forecast table
forecast_table <- data.frame(
Horizon = 1:5,
Forecasted_Volatility = vol_forecast
)
kable(forecast_table, digits = 4,
caption = "GARCH Volatility Forecasts (5 months ahead)",
col.names = c("Months Ahead", "Forecasted Volatility"))
