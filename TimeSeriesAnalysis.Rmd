---
title: "Dynamic Interactions and Volatility Between Stock Returns and Exchange Rates"
author: "Nikolas Tsalidis"
date: "`r Sys.Date()`"
output:
  pdf_document:
    latex_engine: xelatex
    toc: false
  html_document:
    toc: true
    toc_float: true
---
```{r}
rm(list = ls())
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  fig.width = 8,
  fig.height = 6,
  fig.align = 'center'
)
```

```{r load-packages, include=FALSE}
# Load required packages
suppressPackageStartupMessages({
  library(urca)         # Unit root and cointegration tests
  library(vars)         # VAR/VECM models
  library(rugarch)      # GARCH models
  library(fGarch)       # Alternative GARCH
  library(FinTS)        # ARCH-LM tests
  library(forecast)     # ARMA model selection
  library(dplyr)        # Data manipulation
  library(tidyr)        # Data tidying
  library(zoo)          # Time series objects
  library(pracma)       # Detrending
  library(ggplot2)      # Plotting
  library(ggfortify)    # Time series plotting
  library(gridExtra)    # Multiple plots
  library(corrplot)     # Correlation plots
  library(moments)      # Skewness and kurtosis
  library(knitr)        # Tables
})

# Set random seed for reproducibility
set.seed(42)
```

# 1. Introduction

Financial markets are increasingly interconnected in the globalized economy. Understanding the dynamic relationships between stock markets and foreign exchange markets is crucial for investors, policymakers, and risk managers. This study examines the interactions between U.S. stock market returns (S&P 500) and the EUR/USD exchange rate returns over the period January 1999 to November 2025.

## Research Questions

This analysis addresses three key questions:

1. **Do stock returns and exchange rate returns influence each other dynamically?** We investigate whether past movements in one market can predict future movements in the other through impulse response analysis.

2. **How does stock market volatility evolve over time?** We examine whether volatility exhibits clustering and persistence patterns using GARCH models.

3. **Do shocks in exchange rates influence stock market volatility (risk spillovers)?** We explore cross-market volatility transmission through variance decomposition and impulse response functions.

## Economic Motivation

The relationship between stock and foreign exchange markets has important implications. Exchange rate movements affect multinational firms' competitiveness and earnings, influencing stock valuations. Conversely, stock market performance can drive portfolio flows affecting currency demand. Understanding these linkages helps in portfolio diversification, risk management, and monetary policy formulation.

## Methodological Overview

We employ two complementary approaches that address different aspects of financial time series:

**Model 1: Vector Autoregression (VAR)** - We use VAR models to capture the **conditional mean dynamics** and interdependencies between S&P 500 and EUR/USD returns. This bivariate model reveals how past values of both variables jointly predict future returns, addressing Research Questions 1 and 3. The choice between VAR and VECM depends on cointegration test results.

**Model 2: ARMA-GARCH** - We apply univariate ARMA-GARCH models to characterize the **conditional variance dynamics** (time-varying volatility) in S&P 500 returns, addressing Research Question 2. While VAR models the mean, GARCH captures volatility clustering - a stylized fact of financial returns that VAR cannot model.

These two models are complementary: VAR captures how markets influence each other's returns (mean), while GARCH captures how volatility evolves over time (variance). This separation follows standard practice in financial econometrics.

Our analysis follows standard econometric procedures: unit root testing, cointegration analysis, VAR estimation, impulse response analysis, and volatility modeling with GARCH.

---

# 2. Data

## Data Sources

We use two monthly time series covering January 1999 to November 2025:

- **S&P 500 Index**: A broad measure of U.S. stock market performance, representing the 500 largest publicly traded companies.
- **EUR/USD Exchange Rate**: The euro to U.S. dollar exchange rate, the world's most liquid currency pair.

The sample period begins in January 1999 when the euro was introduced, ensuring consistent exchange rate data.

```{r load-data}
# Load S&P 500 data
sp500 <- read.csv("Data/sp500.csv", stringsAsFactors = FALSE)
sp500$Date <- as.Date(sp500$Date, format = "%m/%d/%Y")

# Load EUR/USD data
eurusd <- read.csv("Data/EUR_USD.csv", stringsAsFactors = FALSE)
eurusd$Date <- as.Date(eurusd$Date, format = "%m/%d/%Y")

# Convert EUR/USD from daily to monthly (end-of-month values)
library(zoo)
eurusd_zoo <- zoo(eurusd$Value, order.by = eurusd$Date)
eurusd_monthly <- aggregate(eurusd_zoo, zoo::as.yearmon, tail, 1)
eurusd_monthly_df <- data.frame(
  Date = zoo::as.Date(zoo::as.yearmon(index(eurusd_monthly)), frac = 1),
  Value = as.numeric(coredata(eurusd_monthly))
)

# Filter S&P 500 from 1999 onwards and merge with EUR/USD
sp500_filtered <- sp500 %>%
  filter(Date >= as.Date("1999-01-01"))

data_merged <- inner_join(
  sp500_filtered %>% rename(SP500 = Value),
  eurusd_monthly_df %>% rename(EURUSD = Value),
  by = "Date"
)

# Calculate log returns using differencing
data_merged <- data_merged %>%
  mutate(
    r_SP500 = c(NA, diff(log(SP500))),
    r_EURUSD = c(NA, diff(log(EURUSD)))
  ) %>%
  filter(!is.na(r_SP500))

# Create bivariate time series object
returns_ts <- ts(
  data_merged[, c("r_SP500", "r_EURUSD")],
  start = c(1999, 2),
  frequency = 12
)
```

## Data Transformation

We transform price levels to log returns using differencing:

$$r_t = \Delta \ln(P_t) = \ln(P_t) - \ln(P_{t-1})$$

where $r_t$ is the log return at time $t$, and $P_t$ is the price level. This is consistent with the differencing approach discussed in the lectures for achieving stationarity. Log returns are preferred because they are approximately normally distributed, stationary, and have better statistical properties for time series analysis.

## Summary Statistics

```{r summary-stats}
# Calculate summary statistics
summary_stats <- data.frame(
  Variable = c("S&P 500 Returns", "EUR/USD Returns"),
  Mean = c(mean(returns_ts[, 1], na.rm = TRUE), mean(returns_ts[, 2], na.rm = TRUE)),
  SD = c(sd(returns_ts[, 1], na.rm = TRUE), sd(returns_ts[, 2], na.rm = TRUE)),
  Min = c(min(returns_ts[, 1], na.rm = TRUE), min(returns_ts[, 2], na.rm = TRUE)),
  Max = c(max(returns_ts[, 1], na.rm = TRUE), max(returns_ts[, 2], na.rm = TRUE)),
  Skewness = c(
    skewness(returns_ts[, 1], na.rm = TRUE),
    skewness(returns_ts[, 2], na.rm = TRUE)
  ),
  Kurtosis = c(
    kurtosis(returns_ts[, 1], na.rm = TRUE),
    kurtosis(returns_ts[, 2], na.rm = TRUE)
  )
)

kable(summary_stats, digits = 3, caption = "Summary Statistics of Monthly Returns")
```

The summary statistics reveal several key features. S&P 500 returns average approximately 0.7% per month with volatility around 4.4%, while EUR/USD returns are close to zero with slightly lower volatility. Both series exhibit negative skewness, indicating larger negative returns than positive ones, consistent with financial market behavior during crises. Excess kurtosis (greater than 3) in both series suggests fat tails and the presence of extreme events, justifying the use of GARCH models for volatility.

## Correlation Analysis

```{r correlation}
# Calculate correlation matrix
cor_matrix <- cor(returns_ts, use = "complete.obs")

# Display correlation
kable(cor_matrix, digits = 3, caption = "Correlation Matrix of Returns")
```


The correlation between S&P 500 and EUR/USD returns provides initial evidence of their relationship. A positive correlation would suggest that stock market gains coincide with euro appreciation, while a negative correlation would indicate inverse movements.

## Time Series Plots

```{r plot-sp500-levels}
# Define custom theme
theme_lecture <- theme_light() +
  theme(
    plot.margin = margin(0.2, 0.2, 0.2, 0.2, "cm"),
    text = element_text(size = 12),
    plot.title = element_text(hjust = 0.5, face = "bold"),
    axis.title = element_text(face = "bold")
  )

# Plot S&P 500 levels
ggplot(data_merged, aes(x = Date, y = SP500)) +
  geom_line(color = "steelblue", linewidth = 0.7) +
  labs(
    title = "S&P 500 Index Levels",
    x = "Date",
    y = "Index Value"
  ) +
  theme_lecture
```

```{r plot-sp500-returns}
# Plot S&P 500 returns
ggplot(data_merged, aes(x = Date, y = r_SP500)) +
  geom_line(color = "darkred", linewidth = 0.5) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
  labs(
    title = "S&P 500 Monthly Returns",
    x = "Date",
    y = "Log Returns"
  ) +
  theme_lecture
```

```{r plot-eurusd-levels}
# Plot EUR/USD levels
ggplot(data_merged, aes(x = Date, y = EURUSD)) +
  geom_line(color = "darkgreen", linewidth = 0.7) +
  labs(
    title = "EUR/USD Exchange Rate Levels",
    x = "Date",
    y = "Exchange Rate"
  ) +
  theme_lecture
```

```{r plot-eurusd-returns}
# Plot EUR/USD returns
ggplot(data_merged, aes(x = Date, y = r_EURUSD)) +
  geom_line(color = "darkorange", linewidth = 0.5) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
  labs(
    title = "EUR/USD Monthly Returns",
    x = "Date",
    y = "Log Returns"
  ) +
  theme_lecture
```

The level plots show the long-term trends in both markets. The S&P 500 exhibits sustained growth with notable declines during the 2008 financial crisis and the 2020 COVID-19 pandemic. The EUR/USD exchange rate fluctuated between approximately 0.85 and 1.60, reflecting changing economic fundamentals and monetary policies.

The return plots reveal volatility clustering - periods of high volatility followed by high volatility, and calm periods followed by calm periods. This is particularly evident during crisis periods (2008-2009, early 2020), supporting the need for GARCH modeling. Both series fluctuate around zero, consistent with stationary processes.

---

# 3. Stationarity Analysis

Before estimating VAR or VECM models, we must determine the integration order of each series. We use the Augmented Dickey-Fuller (ADF) test to test for unit roots. Following proper econometric procedure, we first test the original price levels, then test the transformed (log-differenced) series.

## Methodology

**Null Hypothesis (H₀)**: The series has a unit root (non-stationary, I(1))
**Alternative Hypothesis (H₁)**: The series is stationary (I(0))

We follow the sequential testing procedure to determine the appropriate specification:

1. **Start with trend model**: Test using τ₃ and φ₃
   - If τ₃ rejects → I(0), stop
   - If φ₃ significant → trend needed, keep specification
   - If φ₃ not significant → trend not needed, step down

2. **Test with drift only**: Test using τ₂ and φ₁
   - If τ₂ rejects → I(0), stop
   - If φ₁ significant → drift needed, keep specification
   - If φ₁ not significant → drift not needed, step down

3. **Test with no deterministic components**: Test using τ₁
   - If τ₁ rejects → I(0)
   - If τ₁ fails to reject → I(1)

```{r adf-function}
# Sequential ADF testing function with dynamic critical values
# Tests against 1%, then 5%, then 10% critical values sequentially
sequential_adf_test <- function(series, series_name, max_lags = 8) {
  sig_levels <- c("1pct", "5pct", "10pct")

  # Step 1: Test with trend
  adf_trend <- ur.df(series, type = 'trend', lags = max_lags, selectlags = "AIC")
  cat("\n===", series_name, "- ADF Test with Trend ===\n")
  print(summary(adf_trend))

  # Extract test statistics
  tau3 <- adf_trend@teststat[1, "tau3"]
  phi3 <- adf_trend@teststat[1, "phi3"]

  # Check tau3 against critical values sequentially (1% -> 5% -> 10%)
  for (sig in sig_levels) {
    tau3_cv <- adf_trend@cval["tau3", sig]
    if (tau3 < tau3_cv) {
      return(list(conclusion = "I(0)", specification = "trend", test = adf_trend,
                  message = paste("τ₃ =", round(tau3, 3), "< CV =", round(tau3_cv, 3), "(", sig, ") → Reject H₀, series is I(0)")))
    }
  }

  # Check phi3 against critical values sequentially (1% -> 5% -> 10%)
  for (sig in sig_levels) {
    phi3_cv <- adf_trend@cval["phi3", sig]
    if (phi3 > phi3_cv) {
      return(list(conclusion = "I(1)", specification = "trend", test = adf_trend,
                  message = paste("φ₃ =", round(phi3, 3), "> CV =", round(phi3_cv, 3), "(", sig, ") → Trend significant, keep specification. Series is I(1)")))
    }
  }

  phi3_cv_10 <- adf_trend@cval["phi3", "10pct"]
  cat("\nφ₃ =", round(phi3, 3), "< CV =", round(phi3_cv_10, 3), "(10pct) → Trend not needed, stepping down to drift\n")

  # Step 2: Test with drift only
  adf_drift <- ur.df(series, type = 'drift', lags = max_lags, selectlags = "AIC")
  cat("\n===", series_name, "- ADF Test with Drift ===\n")
  print(summary(adf_drift))

  tau2 <- adf_drift@teststat[1, "tau2"]
  phi1 <- adf_drift@teststat[1, "phi1"]

  # Check tau2 against critical values sequentially (1% -> 5% -> 10%)
  for (sig in sig_levels) {
    tau2_cv <- adf_drift@cval["tau2", sig]
    if (tau2 < tau2_cv) {
      return(list(conclusion = "I(0)", specification = "drift", test = adf_drift,
                  message = paste("τ₂ =", round(tau2, 3), "< CV =", round(tau2_cv, 3), "(", sig, ") → Reject H₀, series is I(0)")))
    }
  }

  # Check phi1 against critical values sequentially (1% -> 5% -> 10%)
  for (sig in sig_levels) {
    phi1_cv <- adf_drift@cval["phi1", sig]
    if (phi1 > phi1_cv) {
      return(list(conclusion = "I(1)", specification = "drift", test = adf_drift,
                  message = paste("φ₁ =", round(phi1, 3), "> CV =", round(phi1_cv, 3), "(", sig, ") → Drift significant, keep specification. Series is I(1)")))
    }
  }

  phi1_cv_10 <- adf_drift@cval["phi1", "10pct"]
  cat("\nφ₁ =", round(phi1, 3), "< CV =", round(phi1_cv_10, 3), "(10pct) → Drift not needed, stepping down to none\n")

  # Step 3: Test with no deterministic components
  adf_none <- ur.df(series, type = 'none', lags = max_lags, selectlags = "AIC")
  cat("\n===", series_name, "- ADF Test with None ===\n")
  print(summary(adf_none))

  tau1 <- adf_none@teststat[1, "tau1"]

  # Check tau1 against critical values sequentially (1% -> 5% -> 10%)
  for (sig in sig_levels) {
    tau1_cv <- adf_none@cval["tau1", sig]
    if (tau1 < tau1_cv) {
      return(list(conclusion = "I(0)", specification = "none", test = adf_none,
                  message = paste("τ₁ =", round(tau1, 3), "< CV =", round(tau1_cv, 3), "(", sig, ") → Reject H₀, series is I(0)")))
    }
  }

  tau1_cv_10 <- adf_none@cval["tau1", "10pct"]
  return(list(conclusion = "I(1)", specification = "none", test = adf_none,
              message = paste("τ₁ =", round(tau1, 3), "> CV =", round(tau1_cv_10, 3), "(10pct) → Fail to reject H₀, series is I(1)")))
}

# Sequential ADF test for returns (starts from drift, skips trend)
# Used for already-differenced data where trend was rejected in levels
# Tests against 1%, then 5%, then 10% critical values sequentially
sequential_adf_test_returns <- function(series, series_name, max_lags = 8) {
  sig_levels <- c("1pct", "5pct", "10pct")

  # Step 1: Test with drift (skip trend for differenced data)
  adf_drift <- ur.df(series, type = 'drift', lags = max_lags, selectlags = "AIC")
  cat("\n===", series_name, "- ADF Test with Drift ===\n")
  print(summary(adf_drift))

  tau2 <- adf_drift@teststat[1, "tau2"]
  phi1 <- adf_drift@teststat[1, "phi1"]

  # Check tau2 against critical values sequentially (1% -> 5% -> 10%)
  for (sig in sig_levels) {
    tau2_cv <- adf_drift@cval["tau2", sig]
    if (tau2 < tau2_cv) {
      return(list(conclusion = "I(0)", specification = "drift", test = adf_drift,
                  message = paste("τ₂ =", round(tau2, 3), "< CV =", round(tau2_cv, 3), "(", sig, ") → Reject H₀, series is I(0)")))
    }
  }

  # Check phi1 against critical values sequentially (1% -> 5% -> 10%)
  for (sig in sig_levels) {
    phi1_cv <- adf_drift@cval["phi1", sig]
    if (phi1 > phi1_cv) {
      return(list(conclusion = "I(1)", specification = "drift", test = adf_drift,
                  message = paste("φ₁ =", round(phi1, 3), "> CV =", round(phi1_cv, 3), "(", sig, ") → Drift significant, keep specification. Series is I(1)")))
    }
  }

  phi1_cv_10 <- adf_drift@cval["phi1", "10pct"]
  cat("\nφ₁ =", round(phi1, 3), "< CV =", round(phi1_cv_10, 3), "(10pct) → Drift not needed, stepping down to none\n")

  # Step 2: Test with no deterministic components
  adf_none <- ur.df(series, type = 'none', lags = max_lags, selectlags = "AIC")
  cat("\n===", series_name, "- ADF Test with None ===\n")
  print(summary(adf_none))

  tau1 <- adf_none@teststat[1, "tau1"]

  # Check tau1 against critical values sequentially (1% -> 5% -> 10%)
  for (sig in sig_levels) {
    tau1_cv <- adf_none@cval["tau1", sig]
    if (tau1 < tau1_cv) {
      return(list(conclusion = "I(0)", specification = "none", test = adf_none,
                  message = paste("τ₁ =", round(tau1, 3), "< CV =", round(tau1_cv, 3), "(", sig, ") → Reject H₀, series is I(0)")))
    }
  }

  tau1_cv_10 <- adf_none@cval["tau1", "10pct"]
  return(list(conclusion = "I(1)", specification = "none", test = adf_none,
              message = paste("τ₁ =", round(tau1, 3), "> CV =", round(tau1_cv_10, 3), "(10pct) → Fail to reject H₀, series is I(1)")))
}
```

## Step 1: ADF Test on Original Price Levels

We first test the original price series (S&P 500 index and EUR/USD exchange rate) to determine if they contain unit roots.

### S&P 500 Levels

```{r adf-sp500-levels}
result_sp500_levels <- sequential_adf_test(data_merged$SP500, "S&P 500 Levels")
cat("\n*** CONCLUSION:", result_sp500_levels$message, "***\n")
```

### EUR/USD Levels

```{r adf-eurusd-levels}
result_eurusd_levels <- sequential_adf_test(data_merged$EURUSD, "EUR/USD Levels")
cat("\n*** CONCLUSION:", result_eurusd_levels$message, "***\n")
```

### Interpretation of Level Tests

The sequential ADF testing procedure on the original price levels shows that we fail to reject the null hypothesis of a unit root for both series. After stepping through the specifications (trend → drift → none), the φ tests indicate that neither trend nor drift are necessary. Both the S&P 500 index and EUR/USD exchange rate are non-stationary I(1) processes. This is typical for financial price series, which tend to follow random walk processes.

## Step 2: ADF Test on Log-Differenced Series

Since the original series are I(1), we apply the log-differencing transformation $\Delta \ln(P_t) = \ln(P_t) - \ln(P_{t-1})$ and test the transformed series for stationarity.

**Note:** For the log returns, we start the sequential procedure from `drift` rather than `trend`. This is because we already established from testing the original levels that the trend component was not significant (φ₃ was not significant). Since log returns are differenced data, a deterministic trend is not expected.

### S&P 500 Log Returns

```{r adf-sp500-returns}
result_sp500_returns <- sequential_adf_test_returns(returns_ts[, "r_SP500"], "S&P 500 Log Returns")
cat("\n*** CONCLUSION:", result_sp500_returns$message, "***\n")
```
Data is now stationary! And the dirft can be used

### EUR/USD Log Returns

```{r adf-eurusd-returns}
result_eurusd_returns <- sequential_adf_test_returns(returns_ts[, "r_EURUSD"], "EUR/USD Log Returns")
cat("\n*** CONCLUSION:", result_eurusd_returns$message, "***\n")
```
Data is now stationary! And the dirft can be used



The sequential testing procedure establishes:

1. **Original price levels**: Both S&P 500 and EUR/USD are I(1) - non-stationary with unit roots
2. **Log-differenced series**: Both series become I(0) - stationary after applying $\Delta \ln(P_t)$

This confirms that the log-differencing transformation is appropriate and necessary for achieving stationarity. We proceed with VAR modeling using the stationary log returns series

---

# 4. Cointegration Analysis

Since both original price series are I(1), we can test for cointegration. If the series are cointegrated, there exists a long-run equilibrium relationship between them, and a linear combination of the two I(1) series will be I(0). Cointegration implies that although individual series may wander (unit root behavior), they move together over time and do not drift too far apart.

## Visual Inspection of Log Price Levels

Before conducting formal tests, we plot both I(1) series (log price levels) to visually inspect whether they appear to move together over time.

```{r visual-cointegration, fig.cap="Log Price Levels: Visual Inspection for Cointegration"}
# Create log of price levels (the I(1) series)
log_sp500 <- log(data_merged$SP500)
log_eurusd <- log(data_merged$EURUSD)

# Create time series objects
log_sp500_ts <- ts(log_sp500, start = c(1999, 2), frequency = 12)
log_eurusd_ts <- ts(log_eurusd, start = c(1999, 2), frequency = 12)

# Combine into multivariate time series
log_levels <- ts.intersect(log_sp500_ts, log_eurusd_ts)

# Plot both series together
ts.plot(log_levels, lty = c(1, 2), lwd = c(2, 2),
        col = c("steelblue", "darkred"),
        xlab = "Year", ylab = "Log Price Level",
        main = "Log Price Levels: S&P 500 and EUR/USD")
legend("topleft", legend = c("log(S&P 500)", "log(EUR/USD)"),
       col = c("steelblue", "darkred"), lty = c(1, 2), lwd = 2)
```

The plot shows the evolution of both log price levels over time. For cointegration to exist, we would expect the series to share common stochastic trends - that is, they should not drift apart indefinitely. Visual inspection provides initial intuition, but formal testing is required to draw conclusions.

## Engle-Granger Residual-Based Test for Cointegration

Following the methodology from lecture 8, we use the Engle-Granger two-step procedure to test for cointegration:

**Step 1**: Estimate the long-run relationship using OLS:
$$Y_t = \alpha + \theta X_t + Z_t$$

**Step 2**: Test the residuals $\hat{Z}_t$ for stationarity using an ADF test. If the residuals are I(0), the series are cointegrated.

**Important**: The critical values for testing cointegration residuals are NOT the standard ADF critical values. We use MacKinnon critical values adjusted for the cointegration context.

```{r engle-granger-test}
# Step 1: Estimate long-run relationship using OLS on log levels (I(1) series)
log_sp500 <- log(data_merged$SP500)
log_eurusd <- log(data_merged$EURUSD)

coint_reg <- lm(log_sp500 ~ log_eurusd)

# Display regression results
cat("=== Step 1: Estimate Long-Run Relationship ===\n")
cat("Regression: log(S&P 500) = α + θ × log(EUR/USD) + Z\n\n")
print(summary(coint_reg))

# Extract coefficients
alpha_coint <- coef(coint_reg)[1]
theta_coint <- coef(coint_reg)[2]

cat("\nEstimated long-run relationship:\n")
cat("log(S&P 500) =", round(alpha_coint, 4), "+", round(theta_coint, 4), "× log(EUR/USD)\n")
```

```{r coint-residuals-plot, fig.cap="Cointegrating Residuals from Engle-Granger Regression"}
# Extract residuals
residuals_coint <- coint_reg$residuals

# Plot residuals
plot(data_merged$Date, residuals_coint, type = "l",
     main = "Cointegrating Residuals",
     xlab = "Date", ylab = "Residuals", col = "darkblue", lwd = 1.5)
abline(h = 0, lty = 2, col = "red")
```

If the series are cointegrated, the residuals should appear stationary - fluctuating around zero without a clear trend or persistent wandering.

```{r adf-residuals-test}
# Step 2: Test residuals for unit root (Engle-Granger test)
# For cointegration residuals, we use type = "none" (no intercept or trend)
# because the residuals have zero mean by construction

cat("=== Step 2: ADF Test on Cointegrating Residuals ===\n\n")

adf_residuals <- ur.df(residuals_coint, type = "none", lags = 8, selectlags = "AIC")
print(summary(adf_residuals))

# Extract test statistic
tau_residuals <- adf_residuals@teststat[1, "tau1"]

# Get sample size for MacKinnon response surface formula
T_sample <- length(residuals_coint)

# MacKinnon (2010) response surface coefficients for cointegration
# k=2 variables, no trend (case 1: no constant in cointegrating regression)
# Formula: CV = beta_inf + beta_1/T + beta_2/T^2
mackinnon_coef <- data.frame(
  level = c("1%", "5%", "10%"),
  beta_inf = c(-3.9001, -3.3377, -3.0462),
  beta_1 = c(-10.534, -5.967, -4.069),
  beta_2 = c(-30.03, -8.98, -5.73)
)

# Compute sample-size adjusted critical values
cv_1pct <- mackinnon_coef$beta_inf[1] + mackinnon_coef$beta_1[1]/T_sample + mackinnon_coef$beta_2[1]/T_sample^2
cv_5pct <- mackinnon_coef$beta_inf[2] + mackinnon_coef$beta_1[2]/T_sample + mackinnon_coef$beta_2[2]/T_sample^2
cv_10pct <- mackinnon_coef$beta_inf[3] + mackinnon_coef$beta_1[3]/T_sample + mackinnon_coef$beta_2[3]/T_sample^2

mackinnon_cv <- data.frame(
  Significance = c("1%", "5%", "10%"),
  Critical_Value = c(cv_1pct, cv_5pct, cv_10pct)
)

cat("\n=== Cointegration Test Results ===\n")
cat("Sample size T =", T_sample, "\n")
cat("ADF test statistic on residuals (τ):", round(tau_residuals, 4), "\n\n")
cat("MacKinnon Critical Values (adjusted for sample size):\n")
cat("Formula: CV = β∞ + β₁/T + β₂/T²\n\n")
print(mackinnon_cv, row.names = FALSE)

# Determine conclusion using computed critical values
if (tau_residuals < cv_1pct) {
  coint_conclusion <- paste0("Reject H₀ at 1% level: Series ARE cointegrated (τ = ",
                              round(tau_residuals, 4), " < ", round(cv_1pct, 4), ")")
  coint_result <- "cointegrated"
} else if (tau_residuals < cv_5pct) {
  coint_conclusion <- paste0("Reject H₀ at 5% level: Series ARE cointegrated (τ = ",
                              round(tau_residuals, 4), " < ", round(cv_5pct, 4), ")")
  coint_result <- "cointegrated"
} else if (tau_residuals < cv_10pct) {
  coint_conclusion <- paste0("Reject H₀ at 10% level: Series ARE cointegrated (τ = ",
                              round(tau_residuals, 4), " < ", round(cv_10pct, 4), ")")
  coint_result <- "cointegrated"
} else {
  coint_conclusion <- paste0("Fail to reject H₀: Series are NOT cointegrated (τ = ",
                              round(tau_residuals, 4), " > ", round(cv_10pct, 4), ")")
  coint_result <- "not cointegrated"
}

cat("\n*** CONCLUSION:", coint_conclusion, "***\n")
```

## Interpretation

The Engle-Granger test examines whether a stable long-run relationship exists between log(S&P 500) and log(EUR/USD).

**If the series are cointegrated** (residuals are stationary):
- A long-run equilibrium relationship exists between the two markets
- Deviations from equilibrium are temporary and self-correcting
- We should use a Vector Error Correction Model (VECM) that incorporates the error correction term

**If the series are NOT cointegrated** (residuals are non-stationary):
- No stable long-run relationship exists
- The series may drift apart without bound
- We proceed with a standard VAR model on the stationary returns (first differences)

Based on the Engle-Granger test results, we proceed with a **VAR model on the stationary log returns** since this is appropriate regardless of the cointegration outcome - we are modeling returns (I(0)) rather than levels (I(1)).

---

# 5. VAR Model Estimation

## Lag Selection

Selecting the appropriate lag length is crucial for VAR estimation. We use multiple information criteria to determine the optimal lag.

```{r var-lag-selection}
# Select optimal lag
var_select <- VARselect(returns_ts, lag.max = 8, type = "const")

# Display information criteria
kable(t(var_select$criteria), digits = 3,
      caption = "VAR Lag Selection Criteria",
      col.names = c("AIC", "HQ", "SC", "FPE"))

# Extract optimal lag using AIC
p_optimal <- var_select$selection["AIC(n)"]
var_select$selection
p_optimal
```

## Interpretation of Lag Selection

The AIC (Akaike Information Criterion) balances model fit with parsimony. We use the AIC-selected lag length for our VAR model, as it tends to perform well in forecasting applications. The other criteria (HQ, SC, FPE) provide robustness checks. If criteria disagree substantially, we would conduct sensitivity analysis with different lag lengths. VAR models have many parameters, so overfitting is a real risk - so we should consuder using few lags.

## VAR Estimation

```{r var-estimation}
# Estimate VAR model
var_model <- VAR(returns_ts, p = p_optimal, type = "const")

# Display summary
summary(var_model)
```

## Interpretation of VAR Results

The VAR model estimates two equations simultaneously:

**Equation 1: S&P 500 Returns**
- **Lagged S&P 500 effects**: Coefficients on lagged S&P 500 returns capture autocorrelation (momentum or mean reversion)
- **Lagged EUR/USD effects**: Coefficients on lagged EUR/USD returns capture cross-market influences
- **R²**: Proportion of variance explained by the model
- **F-statistic**: Overall model significance

**Equation 2: EUR/USD Returns**
- **Lagged EUR/USD effects**: Autocorrelation in exchange rate returns
- **Lagged S&P 500 effects**: Stock market influence on currency movements
- **R²**: Model fit quality
- **F-statistic**: Joint significance

The coefficients reveal the dynamic structure: positive coefficients indicate reinforcing effects (momentum), while negative coefficients suggest mean reversion or offsetting relationships. Statistical significance (p < 0.05) indicates reliable predictive relationships.

```{r var-coefficients}
# Extract key statistics for summary table
r2_sp500 <- summary(var_model)$varresult$r_SP500$r.squared
r2_eurusd <- summary(var_model)$varresult$r_EURUSD$r.squared
adj_r2_sp500 <- summary(var_model)$varresult$r_SP500$adj.r.squared
adj_r2_eurusd <- summary(var_model)$varresult$r_EURUSD$adj.r.squared
fstat_sp500 <- summary(var_model)$varresult$r_SP500$fstatistic[1]
fstat_eurusd <- summary(var_model)$varresult$r_EURUSD$fstatistic[1]

var_summary <- data.frame(
  Equation = c("S&P 500 Returns", "EUR/USD Returns"),
  R_squared = c(r2_sp500, r2_eurusd),
  Adj_R_squared = c(adj_r2_sp500, adj_r2_eurusd),
  F_statistic = c(fstat_sp500, fstat_eurusd)
)

kable(var_summary, digits = 3,
      caption = "VAR Model Summary Statistics",
      col.names = c("Equation", "R²", "Adj. R²", "F-statistic"))
```

The summary statistics indicate the overall explanatory power of the VAR system. Higher R² values suggest that past values of both variables effectively predict current returns. The F-statistics test whether all coefficients (excluding the constant) are jointly zero, providing a test of overall model relevance.

---

# 6. Impulse Response Analysis

Impulse response functions (IRFs) trace the dynamic response of one variable to a shock in another variable over time. They provide insight into the persistence and magnitude of cross-market effects.

## Orthogonalized IRFs

We compute orthogonalized IRFs using Cholesky decomposition. The ordering matters: the variable listed first is assumed to affect others contemporaneously. We use the ordering: EUR/USD, S&P 500, assuming the currency market is more exogenous.

```{r irf-computation}
# Compute orthogonalized IRFs
#irf_ortho <- irf(
#  var_model,
#  n.ahead = 12,
#  ortho = TRUE,
#  boot = TRUE,
#  runs = 1000,
#  ci = 0.95
#)
```

```{r irf-plot, fig.width=10, fig.height=8, fig.cap="Orthogonalized Impulse Response Functions (95% Bootstrap CI)"}
# Plot IRFs
#plot(irf_ortho, main = "")
```

## Reduced-Form IRFs

In contrast to orthogonalized IRFs, reduced-form IRFs shock one residual at a time without accounting for contemporaneous correlations between variables. This provides a complementary view of the dynamic propagation mechanism.

```{r irf-reduced}
# Compute reduced-form (non-orthogonalized) IRFs
irf_reduced <- irf(
  var_model,
  n.ahead = 12,
  ortho = FALSE,  # Reduced-form: no Cholesky decomposition

  boot = TRUE,
  runs = 1000,
  ci = 0.95
)
```

```{r}
irf_reduced
```

```{r irf-reduced-plot, fig.width=10, fig.height=8, fig.cap="Reduced-Form Impulse Response Functions (95% Bootstrap CI)"}
# Define extract_varirf function (based on Lecture 6 approach)
extract_varirf <- function(x) {
  # Get variable names
  var_names <- names(x$irf)
  n_vars <- length(var_names)
  n_ahead <- nrow(x$irf[[1]])

  # Initialize data frame with period column
  result <- data.frame(period = 0:(n_ahead - 1))

  # Extract IRF values, lower and upper bounds for each combination
  for (impulse in var_names) {
    for (response in colnames(x$irf[[impulse]])) {
      col_name <- paste0("irf_", impulse, "_", response)
      result[[col_name]] <- x$irf[[impulse]][, response]
      result[[paste0("lower_", impulse, "_", response)]] <- x$Lower[[impulse]][, response]
      result[[paste0("upper_", impulse, "_", response)]] <- x$Upper[[impulse]][, response]
    }
  }
  return(result)
}

# Extract IRF data to tidy format
irf_data <- extract_varirf(irf_reduced)

# Create individual plots for each impulse-response combination
p1 <- irf_data %>%
  ggplot(aes(x = period, y = irf_r_SP500_r_SP500,
             ymin = lower_r_SP500_r_SP500, ymax = upper_r_SP500_r_SP500)) +
  geom_hline(yintercept = 0, color = "red") +
  geom_ribbon(fill = "grey", alpha = .2, color = "grey50", linetype = "dashed") +
  geom_line() +
  theme_light() +
  labs(title = "S&P 500 shock → S&P 500", x = "Months", y = "Response")

p2 <- irf_data %>%
  ggplot(aes(x = period, y = irf_r_SP500_r_EURUSD,
             ymin = lower_r_SP500_r_EURUSD, ymax = upper_r_SP500_r_EURUSD)) +
  geom_hline(yintercept = 0, color = "red") +
  geom_ribbon(fill = "grey", alpha = .2, color = "grey50", linetype = "dashed") +
  geom_line() +
  theme_light() +
  labs(title = "S&P 500 shock → EUR/USD", x = "Months", y = "Response")

p3 <- irf_data %>%
  ggplot(aes(x = period, y = irf_r_EURUSD_r_SP500,
             ymin = lower_r_EURUSD_r_SP500, ymax = upper_r_EURUSD_r_SP500)) +
  geom_hline(yintercept = 0, color = "red") +
  geom_ribbon(fill = "grey", alpha = .2, color = "grey50", linetype = "dashed") +
  geom_line() +
  theme_light() +
  labs(title = "EUR/USD shock → S&P 500", x = "Months", y = "Response")

p4 <- irf_data %>%
  ggplot(aes(x = period, y = irf_r_EURUSD_r_EURUSD,
             ymin = lower_r_EURUSD_r_EURUSD, ymax = upper_r_EURUSD_r_EURUSD)) +
  geom_hline(yintercept = 0, color = "red") +
  geom_ribbon(fill = "grey", alpha = .2, color = "grey50", linetype = "dashed") +
  geom_line() +
  theme_light() +
  labs(title = "EUR/USD shock → EUR/USD", x = "Months", y = "Response")

# Combine into 2x2 grid
grid.arrange(p1, p2, p3, p4, ncol = 2)
```

## Comparison: Reduced-Form vs Orthogonalized IRFs

**Reduced-Form IRF (`ortho = FALSE`):**

- Shocks are measured in **1 unit** of the residual
- The residual covariance matrix Σᵤ is **ignored**
- No contemporaneous spillovers at t=0 beyond the shocked variable
- Shows the **pure autoregressive propagation** through VAR lag matrices
- Ordering of variables does **not** matter

**Orthogonalized IRF (`ortho = TRUE`):**

- Uses **Cholesky decomposition** of residual covariance Σᵤ = PP'
- Shocks are measured in **1 standard deviation** of the structural shock
- Accounts for **contemporaneous correlations** between variables
- Ordering matters: first variable is assumed more exogenous (recursive identification)
- Provides economically interpretable **structural shocks**

In our analysis, the ordering (EUR/USD first, S&P 500 second) assumes that exchange rate shocks can affect stock returns contemporaneously, but stock market shocks only affect exchange rates with a lag. This reflects the assumption that foreign exchange markets, being highly liquid and global, may react more quickly to information.

## Interpretation of IRFs

The IRF plots show four key relationships:

**1. S&P 500 shock → S&P 500 response**: Measures own-shock persistence in stock returns. A positive initial response that gradually decays indicates momentum followed by mean reversion.

**2. EUR/USD shock → S&P 500 response**: Captures the effect of exchange rate shocks on stock returns. A significant response indicates cross-market spillovers. The sign reveals whether euro appreciation boosts (positive) or dampens (negative) U.S. stock returns.

**3. S&P 500 shock → EUR/USD response**: Shows how stock market shocks affect the exchange rate. Positive response suggests stock gains lead to euro appreciation (possibly through capital inflows to European assets when U.S. markets rally and investors diversify).

**4. EUR/USD shock → EUR/USD response**: Measures exchange rate shock persistence.

The confidence intervals (shaded regions) indicate statistical significance. If the interval excludes zero, the effect is statistically significant at the 5% level.

**Persistence Analysis**: Examining how long effects last provides insight into market efficiency and adjustment speeds. Rapid decay suggests efficient markets quickly incorporating information, while prolonged effects indicate slower adjustment or fundamental linkages.

These IRFs address **Research Questions 1 and 3**, revealing both the existence and persistence of dynamic interactions and potential risk spillovers between markets.

---

# 7. Volatility Analysis: GARCH Modeling

The VAR model in Section 5 captures the dynamic mean interactions between S&P 500 and EUR/USD returns. However, financial returns typically exhibit time-varying volatility (volatility clustering), where periods of high volatility are followed by high volatility and calm periods by calm periods. The VAR model assumes constant variance and does not capture these patterns.

To model volatility dynamics, we apply a univariate ARMA-GARCH model to S&P 500 returns. This follows standard practice in financial econometrics, where mean dynamics (VAR) and variance dynamics (GARCH) are modeled separately. The multivariate extension (VAR-GARCH) is beyond the scope of this analysis.

Financial returns often exhibit volatility clustering, where large changes are followed by large changes and small changes by small changes. We test for this using ARCH effects and model it using GARCH.

## ARCH-LM Test

We test VAR residuals for autoregressive conditional heteroskedasticity (ARCH effects).

**Null Hypothesis (H₀)**: No ARCH effects (homoskedasticity)
**Alternative Hypothesis (H₁)**: ARCH effects present

```{r arch-test}
# Extract VAR residuals
var_residuals <- residuals(var_model)

# ARCH-LM test for S&P 500 residuals
arch_test_sp500 <- ArchTest(var_residuals[, "r_SP500"], lags = 12)

# ARCH-LM test for EUR/USD residuals
arch_test_eurusd <- ArchTest(var_residuals[, "r_EURUSD"], lags = 12)

# Create results table
arch_results <- data.frame(
  Variable = c("S&P 500 Residuals", "EUR/USD Residuals"),
  Test_Statistic = c(
    arch_test_sp500$statistic,
    arch_test_eurusd$statistic
  ),
  p_value = c(
    arch_test_sp500$p.value,
    arch_test_eurusd$p.value
  ),
  Conclusion = c(
    ifelse(arch_test_sp500$p.value < 0.05, "ARCH effects present", "No ARCH effects"),
    ifelse(arch_test_eurusd$p.value < 0.05, "ARCH effects present", "No ARCH effects")
  )
)

kable(arch_results, digits = 4,
      caption = "ARCH-LM Test Results on VAR Residuals",
      col.names = c("Variable", "Chi-squared", "p-value", "Conclusion"))
```

## Interpretation

The ARCH-LM test results indicate whether volatility clustering is present. If p < 0.05, we reject H₀ and conclude that ARCH effects exist, justifying GARCH modeling. Financial returns typically exhibit ARCH effects due to information arrival patterns, trading behavior, and risk dynamics.

We proceed with GARCH modeling for S&P 500 returns, as stock market volatility is a primary focus of our analysis.

## ARMA Mean Model Selection

Before estimating GARCH, we select an appropriate mean model for S&P 500 returns using automatic ARMA selection.

```{r arma-selection}
# Automatic ARMA selection
arma_model <- auto.arima(
  returns_ts[, "r_SP500"],
  max.p = 20,
  max.q = 20,
  seasonal = FALSE,
  stationary = TRUE,
  ic = "aic"
)

# Extract ARMA orders
p_arma <- arma_model$arma[1]
q_arma <- arma_model$arma[2]

cat("Selected ARMA order: ARMA(", p_arma, ",", q_arma, ")\n", sep = "")
```

The selected ARMA order provides the mean equation for the GARCH model. This ensures we model both the conditional mean and conditional variance appropriately.

## GARCH(1,1) Specification and Estimation

The GARCH(1,1) model specifies time-varying conditional variance:

$$\sigma_t^2 = \omega + \alpha_1 \varepsilon_{t-1}^2 + \beta_1 \sigma_{t-1}^2$$

where:
- $\omega > 0$: Unconditional variance component
- $\alpha_1 \geq 0$: ARCH effect (reaction to past shocks)
- $\beta_1 \geq 0$: GARCH effect (volatility persistence)
- Stationarity requires: $\alpha_1 + \beta_1 < 1$

```{r garch-estimation}
# Specify GARCH(1,1) model
garch_spec <- ugarchspec(
  variance.model = list(
    model = "sGARCH",
    garchOrder = c(1, 1)
  ),
  mean.model = list(
    armaOrder = c(p_arma, q_arma),
    include.mean = TRUE
  ),
  distribution.model = "norm"
)

# Estimate GARCH model
garch_fit <- ugarchfit(
  spec = garch_spec,
  data = returns_ts[, "r_SP500"]
)

# Display results
garch_fit

# Extract coefficients
garch_coef <- coef(garch_fit)
alpha0 <- garch_coef["omega"]
alpha1 <- garch_coef["alpha1"]
beta1 <- garch_coef["beta1"]
persistence <- alpha1 + beta1

# Calculate half-life of volatility shock
half_life <- log(0.5) / log(persistence)


safe_extract <- function(mat, param, col) {
  if (!is.null(mat) &&
      param %in% rownames(mat) &&
      col %in% colnames(mat)) {
    return(mat[param, col])
  } else {
    return(NA)
  }
}



coef_mat <- garch_fit@fit$matcoef

garch_param_table <- data.frame(
  Parameter = c("ω (omega)", "α₁ (alpha1)", "β₁ (beta1)", 
                "Persistence (α₁ + β₁)", "Half-life (months)"),
  
  Estimate = c(alpha0, alpha1, beta1, persistence, half_life),
  
  Std_Error = c(
    safe_extract(coef_mat, "omega", "Std. Error"),
    safe_extract(coef_mat, "alpha1", "Std. Error"),
    safe_extract(coef_mat, "beta1", "Std. Error"),
    NA,
    NA
  ),
  
  t_value = c(
    safe_extract(coef_mat, "omega", "t value"),
    safe_extract(coef_mat, "alpha1", "t value"),
    safe_extract(coef_mat, "beta1", "t value"),
    NA,
    NA
  ),
  
  p_value = c(
    safe_extract(coef_mat, "omega", "Pr(>|t|)"),
    safe_extract(coef_mat, "alpha1", "Pr(>|t|)"),
    safe_extract(coef_mat, "beta1", "Pr(>|t|)"),
    NA,
    NA
  )
)


```

## Interpretation of GARCH Parameters

**ω (omega)**: The constant term in the variance equation, contributing to long-run average volatility.

**α₁ (alpha1) - ARCH Effect**: Measures sensitivity to recent shocks. Higher α₁ indicates that unexpected returns have a stronger impact on volatility. This captures the "news impact" - how new information affects market uncertainty.

**β₁ (beta1) - GARCH Effect**: Captures volatility persistence or "memory." Higher β₁ means past volatility strongly influences current volatility, indicating long-lasting volatility clusters.

**Persistence (α₁ + β₁)**: The sum measures overall volatility persistence. Values close to 1 indicate highly persistent volatility shocks. The persistence value provides the decay rate: a value of 0.95 means volatility shocks decay at 5% per period.

**Half-life**: The number of months for a volatility shock to decay by half. Calculated as ln(0.5)/ln(α₁ + β₁). Longer half-lives indicate sustained volatility episodes.

The stationarity condition α₁ + β₁ < 1 ensures that volatility does not explode over time. Values very close to 1 (e.g., > 0.98) suggest near-integrated GARCH (IGARCH), indicating extremely persistent volatility.

## Conditional Volatility Plot

```{r garch-volatility-plot}
# Extract conditional volatility (sigma_t)
cond_vol <- sigma(garch_fit)

# Create data frame for plotting
vol_data <- data.frame(
  Date = data_merged$Date[-1],
  Volatility = as.numeric(cond_vol[-1])
)


# Plot conditional volatility
ggplot(vol_data, aes(x = Date, y = Volatility)) +
  geom_line(color = "darkred", linewidth = 0.6) +
  labs(
    title = "Conditional Volatility of S&P 500 Returns",
    x = "Date",
    y = "Conditional Standard Deviation"
  ) +
  theme_lecture
```

## Interpretation of Volatility Dynamics

The conditional volatility plot reveals several important features:

**Volatility Clusters**: Clear periods of elevated volatility followed by calm periods, confirming the GARCH model's relevance.

**Crisis Episodes**: Major spikes correspond to:
- **2008-2009**: Global financial crisis - massive volatility spike
- **2020**: COVID-19 pandemic - sharp but brief volatility surge
- **Other events**: European debt crisis, market corrections, policy uncertainty

**Asymmetry**: Stock market volatility often exhibits leverage effects (negative returns increase volatility more than positive returns). While standard GARCH(1,1) does not capture asymmetry, the plot may reveal whether volatility spikes coincide with market declines.

**Mean Reversion**: Volatility eventually returns to its long-run average, consistent with α₁ + β₁ < 1. The speed of mean reversion depends on the persistence parameter.

This analysis directly addresses **Research Question 2**: How does stock market volatility evolve over time? The GARCH model quantifies volatility persistence and clustering, crucial for risk management and portfolio decisions.

## GARCH Diagnostics

We test whether the GARCH model successfully removes ARCH effects from standardized residuals.

```{r garch-diagnostics}
# Extract standardized residuals
std_residuals <- residuals(garch_fit, standardize = TRUE)

# ARCH-LM test on standardized residuals
arch_test_std <- ArchTest(as.numeric(std_residuals), lags = 12)

cat("ARCH-LM test on standardized residuals:\n")
cat("Chi-squared statistic:", arch_test_std$statistic, "\n")
cat("p-value:", arch_test_std$p.value, "\n")
cat("Conclusion:", ifelse(arch_test_std$p.value > 0.05,
                          "No remaining ARCH effects (model adequate)",
                          "ARCH effects remain (consider alternative specification)"), "\n")
```

If the ARCH-LM test on standardized residuals yields p > 0.05, the GARCH model successfully captures volatility dynamics. Remaining ARCH effects would suggest the need for alternative specifications (e.g., higher-order GARCH, EGARCH for asymmetry, or GJR-GARCH for threshold effects).

## Volatility Forecasting

```{r garch-forecast}
# Forecast volatility 5 periods ahead
garch_forecast <- ugarchforecast(
  fitORspec = garch_fit,
  n.ahead = 5
)

# Extract forecasted volatility
vol_forecast <- as.numeric(sigma(garch_forecast))

# Create forecast table
forecast_table <- data.frame(
  Horizon = 1:5,
  Forecasted_Volatility = vol_forecast
)

kable(forecast_table, digits = 4,
      caption = "GARCH Volatility Forecasts (5 months ahead)",
      col.names = c("Months Ahead", "Forecasted Volatility"))
```

The volatility forecasts provide forward-looking estimates of market uncertainty. In GARCH models, forecasts converge to the long-run unconditional volatility as the horizon increases, with the speed of convergence determined by persistence. These forecasts are valuable for:

- **Risk Management**: Value-at-Risk (VaR) calculations
- **Portfolio Allocation**: Adjusting weights based on expected volatility
- **Option Pricing**: Implied volatility comparisons
- **Trading Strategies**: Volatility timing and hedging decisions

---

# 8. Conclusions

This study examined the dynamic interactions between S&P 500 stock returns and EUR/USD exchange rate returns using monthly data from January 1999 to November 2025. We applied Vector Autoregression (VAR) models to capture mean dynamics and GARCH models to analyze volatility patterns.

## Answers to Research Questions

**1. Do stock returns and exchange rate returns influence each other dynamically?**

Based on VAR analysis, we find evidence of dynamic interactions between the markets. The VAR coefficient estimates reveal cross-market predictive relationships - whether past returns in one market help predict future returns in the other. Significant cross-equation coefficients indicate information flow between stock and currency markets, consistent with integrated global financial markets.

The impulse response functions complement these findings by tracing the persistence and magnitude of cross-market effects. The IRFs show how shocks propagate across markets over time, revealing whether effects are transitory (disappearing within 1-2 months) or persistent (lasting many months).

Economically, these relationships reflect multiple channels: exchange rate movements affect multinational corporate earnings and competitiveness, influencing stock valuations; conversely, stock market performance drives international portfolio flows, affecting currency demand. The strength of these linkages has important implications for portfolio diversification - if markets are highly interconnected, diversification benefits diminish.

**2. How does stock market volatility evolve over time?**

The GARCH(1,1) model reveals strong evidence of time-varying volatility in S&P 500 returns. The estimated parameters indicate:

- **ARCH parameter (α₁)**: Captures sensitivity to recent shocks, showing how quickly volatility responds to new information
- **GARCH parameter (β₁)**: Measures persistence, indicating how long volatility episodes last
- **Overall persistence (α₁ + β₁)**: Quantifies volatility clustering strength

The high persistence value confirms that volatility shocks decay slowly, creating extended periods of elevated or subdued volatility. The calculated half-life provides a concrete measure of shock duration.

The conditional volatility plot clearly illustrates volatility clustering, with notable spikes during crisis periods (2008 financial crisis, 2020 COVID-19 pandemic). These episodes of extreme uncertainty alternate with tranquil periods, confirming that market risk varies substantially over time. This has critical implications for risk management, as historical volatility alone cannot capture time-varying risk. GARCH forecasts enable forward-looking risk assessment for portfolio allocation, hedging strategies, and regulatory capital requirements.

**3. Do shocks in exchange rates influence stock market volatility (risk spillovers)?**

The impulse response analysis provides insights into risk spillovers between markets. The IRFs trace how exchange rate shocks affect stock returns, revealing both the direction and persistence of cross-market impacts. Significant and persistent responses would indicate meaningful risk transmission across markets.

Volatility spillovers can occur through multiple channels: exchange rate uncertainty increasing corporate earnings risk, currency crises triggering portfolio rebalancing, or common exposure to global risk factors. The presence of spillovers suggests that risk managers cannot treat markets in isolation - exchange rate hedging becomes important not just for currency exposure, but for managing overall portfolio volatility.

## Key Takeaways

1. **Statistical Finding**: Both return series are stationary I(0), justifying VAR modeling in levels. ARCH effects are present, validating GARCH specification. The VAR and GARCH models successfully capture mean and volatility dynamics.

2. **Economic Implication**: Financial markets are interconnected, with information flowing between stock and currency markets. Volatility exhibits strong clustering and persistence, requiring time-varying risk models rather than constant variance assumptions.

3. **Most Important Result**: The combination of VAR analysis, impulse response functions, and GARCH modeling provides a comprehensive picture of market interdependencies and risk dynamics, essential for modern portfolio management and risk assessment.

## Limitations

Several caveats apply to our analysis:

- **Monthly Frequency**: Using monthly data may obscure high-frequency dynamics and intraday spillovers. Daily or higher-frequency data could reveal additional patterns.

- **Sample Period**: Our sample includes major crises (2008, 2020), which may drive results. Subsample analysis could reveal time-varying relationships.

- **Linear VAR**: The VAR framework assumes linear relationships. Non-linear models (threshold VAR, regime-switching) might capture asymmetries between bull and bear markets.

- **Normal Distribution**: The GARCH model assumes normally distributed innovations. Student-t or skewed distributions might better capture fat tails and asymmetry.

- **Bivariate System**: Including additional variables (interest rates, commodity prices, volatility indices, sentiment indicators) could enrich the analysis and control for omitted variables.

- **Parameter Stability**: We assume constant parameters throughout the sample. Structural breaks or time-varying parameters might provide better fit.

## Implications for Policy and Practice

**For Investors**:
- International portfolio diversification benefits depend on the strength of stock-currency correlations
- Volatility forecasts enable dynamic risk management and position sizing
- Understanding lead-lag relationships can inform tactical allocation decisions
- Cross-market spillovers suggest benefits from hedging currency exposure even in equity portfolios

**For Policymakers**:
- Exchange rate volatility may have significant effects on stock market stability through valuation and portfolio channels
- Financial stability monitoring should account for cross-market linkages
- Monetary policy impacts both currency and equity markets, requiring careful assessment of transmission mechanisms

**For Risk Managers**:
- Time-varying volatility models (GARCH) are essential for accurate Value-at-Risk (VaR) calculations
- Stress testing should incorporate cross-market spillovers
- Volatility clustering implies that risk is state-dependent - current volatility levels inform future risk assessments
- GARCH forecasts provide forward-looking risk metrics superior to historical volatility measures

## Future Research

Potential extensions to this research include:

- **Higher Frequency Data**: Daily or intraday analysis to capture short-term dynamics and information transmission speeds
- **Additional Variables**: Expanding the system to include interest rate differentials, commodity prices, global volatility indices (VIX), and macroeconomic fundamentals
- **Non-linear Models**: Threshold VAR to capture different dynamics in crisis vs. tranquil periods, regime-switching models for time-varying relationships
- **Multivariate GARCH**: DCC-GARCH or BEKK models to explicitly model volatility spillovers and time-varying correlations
- **Asymmetric Volatility**: EGARCH or GJR-GARCH to capture leverage effects (negative returns increasing volatility more than positive returns)
- **Structural VAR**: Identifying structural shocks through economic theory rather than statistical decomposition
- **Out-of-Sample Forecasting**: Evaluating model performance in real-time forecasting applications
- **Extended Sample**: Including emerging market currencies and stock indices to assess global financial integration

---

# 9. References

## Course Materials
- Lecture 2: Vector Autoregression (VAR) Models
- Lecture 6: Unit Roots, Cointegration, and Vector Error Correction Models (VECM)
- Lecture 9: ARCH and GARCH Models for Volatility Modeling

## Textbooks
- Enders, W. (2015). *Applied Econometric Time Series* (4th ed.). Wiley.
- Lütkepohl, H. (2005). *New Introduction to Multiple Time Series Analysis*. Springer.
- Tsay, R. S. (2010). *Analysis of Financial Time Series* (3rd ed.). Wiley.

## Academic Papers
- Manasseh, C. O., Abada, F. C., Okiche, E. L., Okanya, O., Nwakoby, I. C., Offu, P., ... & Alio, F. C. (2019). External debt and exchange rate behaviour in sub-Saharan Africa: A PVAR approach. *Cogent Economics & Finance*, 7(1), 1627164.

## Data Sources
- S&P 500 Index: Standard & Poor's Financial Services LLC
- EUR/USD Exchange Rate: European Central Bank Statistical Data Warehouse

---




